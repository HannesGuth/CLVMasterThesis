\documentclass{article}

% Language setting
% Replace `english' with e.g. `spanish' to change the document language
\usepackage[english]{babel}

% Set page size and margins
% Replace `letterpaper' with `a4paper' for UK/EU standard size
\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

% Useful packages
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{tikz}
\usepackage{array} % for better arrays (eg matrices) in maths
\usepackage{makecell} % for vertical text in table headers
\usepackage{multirow}
\usepackage{comment}
\usepackage{geometry}
\usepackage{booktabs}
\usepackage{adjustbox}
\usepackage{titlesec}
\usepackage{bbm}
\usepackage{enumitem}
\usepackage{subcaption}
\usepackage{alphabeta}
\usepackage{blindtext}
\usepackage{titlesec}
\usepackage{url}
\usepackage{unicode-math}
\usepackage{fancyhdr}
\usepackage{float}
\pagestyle{fancy}
%\pagestyle{empty}
\setcounter{secnumdepth}{5}
\setcounter{tocdepth}{5}

\usepackage{lscape} % for landscape tables

\title{Your Paper}
\author{You}

\begin{document}

\tableofcontents

\section{Introduction}
The objective of every company is to maximize revenue and profits by using the least possible input. Hence, departments inside companies must compete for scarce resources, and so does marketing. Marketing managers have to provide success and justify their budget and their expenditures, e.g. for campaigns or special discounts. A good resource allocation is therefore essential and to achieve this, customers, their potential and uncertainty must be evaluated as accurate as possible. The Customer Lifetime Value, CLV, is a well-known concept and gives information about the potential revenue (and profit) to expect from a customer. However, it is unknown by definition and must therefore be modeled, e.g. with the so-called pnbd (Pareto Negative Binomial Distribution) model. As it is usually the case for models, it is subject to uncertainty in its point predictions what puts marketing managers at risk when blindfold trusting these predictions.\\
This work will address this problem by introducing established methods to derive prediction intervals to the pnbd context and benchmarking them against the well-known Bootstrap method. The benchmarking will be done via several key metrics that are calculated for all methods for real-world data sets. The goal is to identify strengths and weaknesses of every method and assess their usefulness and reliability in practice. A special focus will be laid on Conformal prediction as a relatively young method that has gained a lot attention recently. As a final step, it will be assessed if the derived prediction intervals can help identify especially valuable customers, beyond their use in assessing the uncertainty that is connected with their point forecasts. Throughout the whole work, a proxy for the CLV, the CET (Conditional Expected Transactions) will be used, what can be justified as it is identified as the main driver of the CLV in \cite{1L}. \footnotemark{} \\
This thesis can be seen in both, research and a managerial context.
\footnotetext{This change from CLV to CET has the reason that several methods that derive prediction intervals have in parts already been implemented in other R packages which are used in this work. Those do not include the prediction of spending (per transaction) which would be essential to calculate the CLV. In order to use these packages, this CET proxy is used. With respect to the implementation of the additional methods, this exchange makes not difference. In addition, the performance of the implemented methods to construct prediction intervals, which are not based on external packages, will be included for PTS (Predicted Total Spending) can be made available on request.}

\section{Literature review}

\subsection{Role of uncertainty in marketing}
To get a broader view of uncertainty more general in marketing, the notations of “uncertainty” and “marketing”, both shall be defined first. While there are no generally recognized definitions, there are various attempts to provide a definition. Following Collin’s dictionary, then “Uncertainty is a state of doubt about the future or about what is the right thing to do.” \cite{online1} Hubbard states in the context of business “The lack of complete certainty, that is, the existence of more than one possibility. The 'true' outcome/state/result/value is not known.” (\cite{book1}, p. 84)  Both definitions agree on the presence of unknown information with respect to a current or potential future state and connected actions (to be taken). On the other hand, there is a similar situation for the definition of marketing, the AMA (American Marketing Association) defines it as follows: “Marketing is the activity, set of institutions, and processes for creating, communicating, delivering, and exchanging offerings that have value for customers, clients, partners, and society at large.” \cite{online2}  what makes it a very broad field but having a rough focus on the placement of offerings to clients or alike. The following paragraph shall introduce why uncertainty plays a vital role in marketing and why it needs to be taken into consideration.\\
As its name suggests, Marketing is concerned with the placement of offerings on a market and here is where uncertainty comes into play because an important aspect is the uncertain prospective demand. A market is a place which is heavily concerned with and driven by the actions of its agents, i.e. “customers, clients, partners, and society at large” \cite{online2}, and their coordination. (\cite{22RU}, p. 35) All of those agents come with uncertainty in their actions as they are ruled by human beings who make (ir-) rational or at least (un-) predictable \cite{15RU} decisions, let it be a new product launch, the choice of a campaign, the location of new branch or simply a consumer’s unawareness of a competitor’s product which might be superior to their usual choice. Besides those human-driven uncertainties, estimating demand, and placing offerings successfully in the market is affected by some additional dimensions of uncertainty, e.g. own product quality \cite{16RU} that may vary by changing quality of delivered feedstocks. Competitors may bring unexpected technical advancements or the economic situation for the own product can change due to political conflicts and newly imposed taxes, to the good and bad, both. (\cite{17RU}, \cite{18RU}) Also, if a campaign was launched successfully in one region, it does not imply that it would also work out in another (\cite{19RU}, p. 2). This list could be continued nearly infinitely but should be sufficient to give motivation to why one wants generally to consider uncertainty in marketing decisions. It is obvious that, from a marketing perspective, it is desirable to keep uncertainty as low as possible to make best decisions. Therefore, being able to understand the uncertainty in the specific case, i.e. identify its sources and quantify its amount is essential. An established approach in numerous contexts across science is to make predictions about the future by constructing models which depict a picture of reality, incorporating important aspects and leaving out unimportant ones for simplification. One example is the pnbd which aims to predict future customers buying behavior and is the underlying model in this work.

\subsection{Methodological literature overview}
To continue the thought of the previous paragraph and connect it with the context of this work, first it is to determine where uncertainty may arise in the CET prediction.

\subsubsection{Sources of uncertainty}

\begin{center}
    \hspace*{-1.8cm}
    \begin{tikzpicture}
        % Draw the left circle
        \fill[red, opacity=0.3] (-1,0) circle (6);
        \draw (-1,0) circle (6) 
            node at (-1,5) {\textbf{\Large Aleatory uncertainty}}
            node at (-3.5,0.5) {\begin{minipage}{4cm}
            \begin{align*}
                &\text{Campaigns of competitors \cite{20L}\cite{16L}}\\\\
                &\text{Number of marketing contacts \cite{16L}}\\\\
                &\text{State of the economy \cite{16L}}\\\\
                &\text{Retention and churn \cite{20L}\cite{24L}}
            \end{align*}
            \end{minipage}};
        
        % Draw the right circle
        \fill[blue, opacity=0.3] (6,0) circle (6);
        \draw (6,0) circle (6) 
            node at (6.1,5) {\textbf{\Large Epistemic uncertainty}};

        % Add letters to the right circle
        \node at (8.5,0.75) {\begin{minipage}{4cm}
        \begin{align*}  
        &\text {Generally model related errors \cite{14PI}\cite{9L}}\\
        &\text {e.g. using the wrong model \cite{21RU}}\\\\
        &\text {Parameter estimation errors \cite{14PI}\cite{9L}\cite{27L}}\\
        \end{align*}
        \end{minipage}};
        
        % Add overlay text
        \node at (2.5,0.5) {\begin{minipage}{4cm}
        \begin{align*}
            &\text{Data uncertainty, e.g \cite{9L}\cite{29L}}\\
            &\text{- random variation in the}\\
            &\text{data generating process \cite{14PI}}\\
            &\text{- not enough data used/}\\
            &\text{collected (potential bias) \cite{21RU}}
        \end{align*}
        \end{minipage}};
    \end{tikzpicture}
\end{center}

The sources of uncertainty in models in general can be divided into aleatory and epistemic uncertainty \cite{21RU} which also applies to the pnbd model. As there is, to my knowledge , no clear, distinct definition of these two notations, the idea behind shall be briefly explained. Aleatory uncertainty refers to uncertainty coming from random events \cite{20RU}. It captures noise in the inherent observations and is therefore input dependent \cite{21RU}. The uncertainty that comes from inside the model, i.e. the parameters, is called epistemic uncertainty \cite{21RU}. In addition, it “[…] captures our ignorance about which model generated our collected data” (\cite{21RU}, p. 2). Aleatory uncertainty cannot be reduced by e.g. collecting more data while this would be possible for epistemic uncertainty \cite{21RU}.
In the following, the sources will be discussed in more detail, starting with the aleatory side. Influences found in the literature that increase the uncertainty of customer behavior and hence the CLV do so because they are not fully considered in the model. Examples are campaigns of competitors, marketing contacts (in the past, presence and future) and state of the economy in a sense that people change their consumption behavior between recession and boom times. What is also a dominant issue that creates uncertainty is the possibility of a customer leaving the company forever either to switch to a competitor or stop consuming. The probability of being “alive” is included in the model but still, most customers won’t notify the company when they churn, so it stays a mere probability. The second part considers epistemic sources. Note that the papers quoted here are not necessarily concerned with CLV estimation but treat forecasting models in general or in other contexts, often time series or wind/energy forecasting. Nevertheless, since the pnbd model suffers from similar issues, these aspects are relevant here as well. Especially often addressed in the literature is the parameter estimation which comes with uncertainty. This issue will be addressed later in this work. Also often mentioned is data uncertainty. This part is to locate on the intersection because errors can appear in the data collection and processing (aleatory) and there is a of lack of knowledge about data errors and potential biases (epistemic).

\subsubsection{The importance of prediction intervals}
With these points raised, it is evident that mere point forecasts will be in most situations an insufficient indicator about the future values as they do not provide information about uncertainty (\cite{2PI},\cite{6PI},\cite{9PI},\cite{12PI}). Hence, point estimates are often accompanied or even replaced by so-called confidence intervals (for e.g. parameter estimation) and prediction intervals in the context of forecasts. \cite{5PI} The advantages are discussed in the following. An interval (forecast) is offering a range of possible values of future outcomes. \cite{11PI} This means that the true value of the prediction will fall into this declared interval with a specific probability of p\%. \cite{4PI} point out 4 main points, why interval forecasts, and therefore PIs as well, are of such importance.

\begin{enumerate}
    \item They “assess future uncertainty” (\cite{4PI}, p. 476)
    \item They enable the user to plan “different strategies for the range of possible outcomes” (\cite{4PI}, p. 476). This means that one can prepare a strategy in case a high value inside the interval is realized and one for a low value, or the interval is so narrow and reliable that one can be sure with 95\% that one specific strategy will be appropriate. In the context of CLV, it could help discover customers with high variability in their CLV and therefore target them especially. There are 2 rationales behind this approach: First, \cite{16L} state that there is often a right tail distribution for CLV, and it hence makes sense to target a customer with high variability to realize that potential. Second, \cite{33L} state one should focus on those as it offers the opportunity to learn and reduce uncertainty.
    \item They “compare forecasts from different methods more thoroughly“ (\cite{4PI}, p. 476). This means that PIs provide information about the reliability of each method what can be valuable when choosing methods for specific situations.
    \item PIs ”explore forecasts based on different assumptions more carefully“ (\cite{4PI}, p. 476). When there is for example a method that assumes normal distribution, and one that is similar but does not make this assumption, and they produce different interval lengths, one might want to re-assess a model’s assumptions.
\end{enumerate}

\noindent Another point, made by \cite{32PI} (p. 52) “forecasts cannot be expected to be perfect, and intervals emphasize this” (alternatively: (2PI) 3.5.) which underlines maybe the most important power of PIs, namely pointing out to the user of forecasts that they are most probably wrong and hence treat them appropriately. Thinking one step beyond PIs, a more sophisticated option are density predictions, which are comparable with PIs, but they assign probabilities to each area in the interval and provide even more information about uncertainty. \cite{6PI}

\subsubsection{Methods to derive prediction intervals}
As the importance of PIs is justified now, it shall be introduced how they are obtained. First, it is important to note that different models and contexts can require different methods to derive PIs. In this work, the focus will be put on 33PI, who suggest 4 big classes of methods applicable in regression contexts: Bayesian approach, Ensembles, Direct interval estimation and Conformal prediction. These methods will be introduced in general in this chapter, explained with the concrete implementation in the CLV context in the next chapter and benchmarked against each other. The ensemble method will be implemented in connection with bootstrap which picks up the idea of ensemble methods but is too far away to be purely called “ensemble”. In addition, a pure Bootstrap method is used as benchmark. A special emphasis will be put on conformal prediction due to its recent raise in attention in the statistical community.

\paragraph{Bootstrap method}\mbox{}\\
The Bootstrap method is the method which is used as a benchmark in this work. In this chapter, it will be explained in general before going into the concrete application for the CLV context.\\
Bootstrap is a non-parametric and powerful approach to estimate statistics like a mean or quantiles of distribution and therefore as well PIs. The general approach to conduct a bootstrap is as follows.

\begin{enumerate}[label=Step \arabic*:, leftmargin=2cm]
    \item From a sample of data of size n, draw n times with replacement
    \item Repeat 1. sufficiently often, e.g. 1000 times to create 1000 new samples
    \item For each of these new 1000 samples, calculate the desired metric.
    \item From this distribution of the metric, take the central 90\% of predictions. This is the desired interval.
\end{enumerate}

\noindent The central assumptions for bootstrapping are the following: The initially sampled data, from which the new samples are created, must be representative for the whole population and independent from each other, i.e. they must be i.i.d.

\paragraph{Mini Bootstrap / Ensemble}\mbox{}\\
Ensembles are in general a very straightforward method to derive prediction intervals. They can be described as follows: “An ensemble is a collection of a (finite) number of neural networks or other types of predictors that are trained for the same task. A combination of many different predictors can often improve predictions […]” (\cite{58PI}, p.190) When one has enough fitted models and therefore enough point predictions, one can construct naïve prediction intervals \cite{45PI} or calculate mean and variance, assume a (normal) distribution and derive PIs by calculating the respective z-values for desired quantile \cite{33PI}. \cite{57PI} suggest a special form of this approach, which has characteristics of both, Ensembles and Bootstrap.

\begin{enumerate}[label=Step \arabic*:, leftmargin=2cm]
    \item Fit 1 single model that has several parameters
    \item Derive the covariance matrix of the parameters of this fitted model
    \item Derive a large number of parameter combinations that have the characteristics described in the covariance matrix (one has to make assumptions about their distribution)
    \item Treat these parameter combinations as independent models and make predictions with these models for each record of the data set
    \item Take the naïve prediction intervals for each record
\end{enumerate}

\noindent This approach has significant similarity with the previously described bootstrap approach and therefore, similar performance is to expect. In contrast, only 1 model is fitted and from there, all other models are derived by simply simulating parameters what makes it computationally more attractive.

\paragraph{Bayesian method}\mbox{}\\
The roots of Bayesian statistics go back to the 18th century, to Thomas Bayes, as the name suggests \cite{59PI}. Who was the first one to make use of this approach to construct PIs is hard to tell but one of the pioneers in this field was Aitchison in \cite{55PI} who introduced the idea of using the Bayesian Approach’s strength in forming tolerance regions. The idea is to derive a probability distribution over the parameter(s) and based on this, derive a distribution of the outcomes and take the desired statistics. The process is described in the following and is based on \cite{33PI}, \cite{51PI} and \cite{55PI}.

\begin{enumerate}[label=Step \arabic*:, leftmargin=2cm]
    \item Useful to have information about the parameter probability distribution (prior distribution) before the parameters are observed (or use an uninformative distribution) \cite{55PI}
    \item Necessary to have a likelihood function that describes how likely it is to observe the data that are revealed step by step under the current parameter distribution \cite{33PI}
    \item As more information (data) is revealed, update the prior parameter distribution with the new information to derive the posterior parameter distribution, using Bayes’ rule \cite{33PI}
    \item Predict the outcomes based on the posterior parameter distribution to get a distribution of outcomes (predictive distribution)
    \item Calculate the intervals based on this outcome distribution \cite{33PI}
\end{enumerate}

\noindent Therefore, the Bayesian Approach is a method to estimate parameters and at the same time delivers a distribution of outcomes from where one can derive the prediction interval. Both, the posterior parameter distribution and the posterior prediction distribution, can be retrieved approximately by applying the Markov Chain Monte Carlo Method.

\paragraph{Quantile regression}\mbox{}\\
Quantile regression is the last method that shall be used in the course of this work. It was first introduced by \cite{54PI} in 1978 and is a form of direct interval estimation what means a method that does not model a distribution of outcomes but is designed to directly output an interval. \cite{33PI} \\
Following the process described in (\cite{33PI}, \cite{47PI}, \cite{54PI}), Quantile regression works through optimizing the parameters of a model with respect to a loss function that is employed while fitting. The idea is that one combination of parameters yields a specific number of overestimations and underestimations of outcomes. Aiming e.g. for a symmetric distribution would mean 50\% overestimations and 50\% underestimations (assuming that point predictions never hit the target perfectly). \cite{47PI} state that this is possible with any quantile other than the 50\% quantile as well, as the same principle applies: The loss function penalizes deviation of the desired above-below ratio, which is 1:1 in the 50\% case. Targeting intervals, e.g. a symmetric 90\% interval, would require finding the 5\%- and 95\% quantile. Therefore, two parameter combinations must be found. Applying the same principle as above, the loss function would penalize according to the desired quantiles, yielding the parameter combinations that come closest to the objective.\\
This approach requires interrupting and changing the model fitting procedure. As this would be out of scope for this work, a modified version will be implemented for the CLV context in the Applied Methods Chapter that keeps the core idea but simplifies the procedure. A step-by-step guide for the implementation will also be provided in this later chapter.

\paragraph{Conformal prediction}\mbox{}\\
Conformal prediction, or Conformal Inference is a relatively young method to derive prediction intervals with attractive empirical guarantees and few assumptions about the data and the model form to which it is applied. \cite{1CP} It was first introduced by \cite{23CP} and \cite{24CP} in 1999 and gained a lot of attention in recent years. Conformal prediction has two main forms of implementation, Full conformal prediction (Transductive Conformal Prediction) and Split Conformal prediction (Inductive Conformal Prediction) where Full CP has been developed first and Split CP has emerged as an important special case \cite{1CP} after being initially introduced by \cite{26CP} in 2002. The importance of the split version comes from the high computational costs associated with the full version but also sacrifices statistical efficiency \cite{1CP}. In this work, the focus will be exclusively put on the split version, because of the mentioned computational efficiency but also, which is crucial, because Full CP is not applicable for the pnbd-model. The reason for this shall be briefly outlined with a basic example outside of the pnbd-context, before going into detail with the applied split version.\\
Full conformal prediction\\
Following \cite{1CP} and \cite{9CP}, full conformal prediction is implemented as follows: Assume there are 250 records of a) predictors $X_{i}$ = 1:250 and b) observed outcomes $Y_{i}$ = 1:250. From these 250 records, 1 complete record is taken out. Assuming to not know what the true Y for this record is, one can only state that this outcome lives in the range of all possible future outcomes $\boldsymbol{Y}$. The approach is to take n values as possible outcomes out of $\boldsymbol{Y}$ and reunite each of these n "invented" records with the 249 unchanged ones, ending up again with n sets of records. For each set, a new model is fitted what is computationally costly. Predicting with each of these different models the value that was left out, and applying a score function to this outcome, one ends up with n score values. From here one would go on and create prediction intervals. However, in the context of the pnbd-model, it is not possible to continue because this model does not consider the true outcomes when fitting the model, as it is exclusively focused on the purchase history of customers. Therefore, fitting n models by supplying n different outcomes for Y would not lead to different models and would not allow to form PIs.\\\\
Split conformal prediction\\
Even though CP is applicable to both, regression and classification problems, the focus of this work is by default exclusively on regression.\\
General procedure for of split conformal prediction in regression, following \cite{26CP}, \cite{1CP}, \cite{9CP}:

\begin{enumerate}[label=Step \arabic*:, leftmargin=2cm]
    \item Split the data in training, calibration and test set
    \item Fit the prediction model on the training set
    \item Make predictions with this model on the calibration set
    \item “Identify a heuristic notion of uncertainty […]” (\cite{1CP}, p. 5), e.g. the absolute error of a prediction $|y_{i} - \hat{f}(x_{i})|$
    \item Define a score function (A score function can be chosen arbitrarily if it has the right orientation, i.e. lower values are better) \cite{9CP} and apply this function to the forecasting errors
    \item Compute the quantile as see \cite{1CP}, p.5 of the calibration scores
    \item Make predictions on the test set
    \item Use the previously calculated quantile to form prediction intervals (add/subtract the quantile from the point predictions of the test set)
\end{enumerate}
Regardless of the score function, these intervals have the coverage guarantee, defined in \cite{23CP}, and the concrete formulation taken from \cite{1CP}, p.6

\begin{equation}
    P\left(Y_{test} \in \boldsymbol{C}(X_{test})\right) \ge 1-\alpha
\end{equation}

\noindent The only condition that must hold for this coverage guarantee is exchangeability \cite{1CP} in a sense that records, from training, validation and test (what is being predicted) are exchangeable which is weaker than i.i.d. because exchangeability can be expressed with the following formula, see \cite{9CP}, p.3.
\begin{equation}
    \left(Y_{1},...,Y_{n+1}\right) \overset{d}{=} \left(Y_{\sigma(1)},...,Y_{\sigma(n+1)} \right), \hspace{5pt} for \hspace{2pt} all \hspace{2pt} permutations \hspace{2pt} \sigma
\end{equation}

\noindent This implies that the distribution after splitting the data randomly is expected to be the same in each split.\\
The concrete implementation of CP for the pnbd case will be done in the next chapter.


\subsubsection{Measures to assess reliability and sharpness}
The goal of this work is to assess different methods in their performance to derive prediction intervals and benchmark them against the Bootstrap approach. To fulfill this goal, several measures will be introduced in the following. They mainly address coverage, width and combined performance. The table below informs about recent works which employed these methods and how they are useful.

\paragraph{PICP (Prediction Interval Coverage Probability)} \footnotemark{} \mbox{}\\
This measure holds the percentage of cases when the true value lays inside the constructed PI.
\begin{equation}
    PICP = \frac{1}{n} * \sum_{i=1}^{n} \mathbbm{1} (y_{i} \in PI_{i})
\end{equation}

\footnotetext{Different names have been found in the literature for this measure: \cite{2L}: Coverage; \cite{15PI}: True coverage; \cite{16PI}, \cite{20PI}, \cite{35PI}: PICP; \cite{31PI}: Coverage rate; \cite{38PI}: ECP (Empirical coverage probability)}

\paragraph{ACE (Average Coverage Error)} \footnotemark{} \mbox{}\\
This measure indicates how much on average the Prediction Interval Nominal Confidence, PINC, i.e. 90\% and the true coverage, PICP, differ.

\begin{equation}
    ACE = PINC - PICP
\end{equation}
\footnotetext{Different names have been found in the literature for this measure: \cite{16PI}, \cite{20PI}, \cite{38PI}: ACE; \cite{31PI}: ACD (absolute coverage difference)}
\paragraph{PICPW (Prediction Interval Coverage Probability Weighted)}\mbox{}\\
This measure assesses how the coverage develops for more valuable customers. It weighs the coverage with the number of true transactions and therefore overweighs important customers compared to the neutral PICP. E.g. if there were 1000 repeat purchases across all customers of a dataset, a customer has 15 purchases and the interval covers these 15, this “1” for “value covered” would be weighted with 15/1000. If in contrast a customer makes 0 transactions, it would not increase PICPW, regardless of the interval covering this value because its weight is 0/1000. Hence PICPW only measures if high value customers are identified and not if low value customers are identified. Obviously, it must be evaluated together with the width. Another interpretation of this measure is essentially the coverage of repurchases across the customer base.

\begin{equation}
    PICPW = \sum_{i=1}^{n} \mathbbm{1} (y_{i} \in PI_{i}) * \frac{y_{i}}{\sum_{j=1}^{n} y_{j}}
\end{equation}

\paragraph{PIARW (Prediction Interval Average Relative Width)}\mbox{}\\
This measure assesses the interval width with respect to the level of the estimation. It takes account of the special CET situation where the predictions have significantly different values and hence, intervals must be assessed accordingly. E.g. an interval having the width of 4 is of different value for a prediction of 1.2 and a prediction of 50. The actual PIARW is the mean over all customers.

\begin{equation}
    PIARW = \frac{1}{n} * \sum_{i=1}^{n} \frac{UL_{i}-LL_{i}}{est_{i}}
\end{equation}

\paragraph{PIARWW (Prediction Interval Average Relative Width Weighted)}\mbox{}\\
This measure is the equivalent for PICPW but for PIARW.

\begin{equation}
    PIARWW = \sum_{i=1}^{n} \frac{UL_{i}-LL_{i}}{est_{i}} * \frac{y_{i}}{\sum_{j=1}^{n} y_{j}}
\end{equation}

\paragraph{MSIS (Mean Scaled Interval Score)} \footnotemark{} \mbox{}\\
The MSIS is based on the interval score, proposed by (36PI) but scaled by the estimation and averaged over all customers. In other time-series contexts, see the table below, the scaling is done with the seasonal differences which is not applicable here. This measure assesses reliability and sharpness at the same time because it penalized interval width and true values outside of the CI. A low value is therefore better than a higher one.
\begin{equation}
    MSIS = \frac{1}{n} * \sum_{i=1}^{n} \frac{UL_{i} - LL_{i}}{est_{i}} + \frac{2}{\alpha} * \left(\mathbbm{1} (y_{i} > UL_{i}) * \frac{y_{i} - UL_{i}}{est_{i}} + \mathbbm{1} (y_{i} < LL_{i}) * \frac{LL_{i} - y_{i}}{est_{i}}\right)
\end{equation}
\footnotetext{In the works that used this measure before, it was in a time-series context and scaled by seasonal differences. In the CLV context, this scaling is not applicable and is replaced with a scaling by est.}

\paragraph{SWR (Sharpness Width Ratio)}\mbox{}\\
This measure evaluates coverage per width achieved by the intervals. A higher value is better than a lower.

\begin{equation}
    SWR = \frac{PICP}{MSIW}
\end{equation}

\paragraph{Upper coverage}\mbox{}\\
This measure indicates the percentage of times the upper prediction limit was not exceeded by the true value.

\begin{equation}
    Upper \hspace{3pt} coverage = \frac{1}{n} * \sum_{i=1}^{n} \mathbbm{1}(y_{i} \le UL_{i})
\end{equation}

\paragraph{Lower coverage}\mbox{}\\
This measure indicates the percentage of times the lower prediction limit was smaller than the true value.

\begin{equation}
    Lower \hspace{3pt} coverage = \frac{1}{n} * \sum_{i=1}^{n} \mathbbm{1}(y_{i} \ge LL_{i})
\end{equation}

\paragraph{Computational time}\mbox{}\\
This measure holds the time which was needed by a method to calculate the prediction intervals.

\paragraph{Summary}\mbox{}\\
The following table is not exhaustive in a sense that it does not contain all papers that dealt with evaluating their PIs but contains a collection of recent studies that employed inter alia methods that are applicable for the CET context as well.

\begin{table}[!ht]
    \centering
    \setlength{\tabcolsep}{4pt} % Adjust column separation if needed
    \renewcommand{\arraystretch}{1.2} % Adjust row separation
    \begin{adjustbox}{width=\textwidth} % Adjust table width
    \begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|}
    \hline
        ~ & 
        \cite{2L} & \cite{15PI} & \cite{16PI} & \cite{20PI} & \cite{31PI} & \cite{35PI} & \cite{36PI} & \cite{60PI} & \cite{60PI} & \cite{38PI} & \cite{59PI} & \cite{39PI} & 
        \rotatebox{90}{This work} & 
        \rotatebox{90}{\makecell[l]{Reliability \\ and Sharpness}} & 
        \rotatebox{90}{Reliability} & 
        \rotatebox{90}{Sharpness} & 
        \rotatebox{90}{\makecell[l]{Downside risk}} & 
        \rotatebox{90}{\makecell[l]{Upside \\ potential}} & 
        \rotatebox{90}{\makecell[l]{Context \\ independent \\ Generalizability}} \\ \hline
        
        PICP & x & x & x & x & x & x & ~ & x & x & x & x & ~ & x & ~ & x & ~ & ~ & ~ & x \\ \hline
        ACE[3] & ~ & ~ & x & x & x & ~ & ~ & ~ & ~ & x & ~ & ~ & x & ~ & x & ~ & ~ & ~ & x \\ \hline
        PICPW & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & x & ~ & x & ~ & ~ & x & x \\ \hline
        PIARW & ~ & ~ & ~ & ~ & ~ & ~ & ~ & x & x & ~ & x & ~ & x & ~ & ~ & x & ~ & ~ & ~ \\ \hline
        PIARWW & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & x & ~ & ~ & x & ~ & ~ & ~ \\ \hline
        MSIS[1] & x & ~ & ~ & ~ & x & ~ & x & ~ & ~ & ~ & ~ & x & x & x & ~ & ~ & ~ & ~ & ~ \\ \hline
        SWR & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & x & x & ~ & ~ & ~ & ~ & x \\ \hline
        Upper coverage & x & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & x & ~ & x & ~ & ~ & x & x \\ \hline
        Lower coverage & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & x & ~ & x & ~ & x & ~ & x \\ \hline
        Comp. time & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & x & ~ & ~ & ~ & ~ & ~ & x \\ \hline
    \end{tabular}
    \end{adjustbox}
\end{table}


\section{Main part}

\subsection{Applied methods}
In this chapter, it will be explained how the previously introduced methods to derive PIs are concretely implemented in the pnbd context.\\
A few notes to make before going into detail with the single methods:
\begin{itemize}
    \item For Bootstrap, Mini bootstrap / Ensemble and the Bayesian method, only customer data from the current cohort are used without explicitly mentioning it.
    \item For Quantile regression and Conformal prediction, in addition to the current cohort \footnotemark{}, and old cohort is necessary. It will be clearly indicated at any point which cohort is meant.
    \item Each cohort has a holdout and a prediction period that is the same across methods but can differ across data sets. Find more information on the cohorts in tables \ref{gift data information} to \ref{apparel data information}.
    \item This work uses $\alpha = 0.1$, therefore 90\% prediction intervals. Any other quantile is possible as well.
\end{itemize}
\footnotetext{In practice, the true cohort is the cohort of customers which joined 1 or 2 years ago and for whom their future behavior shall be predicted. In this work, also the current cohorts lay in the past for all data sets so that it can be checked if the methods really form PIs that cover the true data.}

\subsubsection{Bootstrap}
The bootstrap approach is used as benchmark for all other methods to be introduced later, as it is the most established method, spread in all scientific domains and straight-forward to implement and understand. The procedure goes as follows.

\begin{enumerate}[label=Step \arabic*:, leftmargin=2cm]
    \item Create n bootstrap samples of the customers (and their respective transactions)
    \item For each bootstrap sample, fit a new pnbd-model on the training period
    \item With each model, estimate the CET for all customers for the holdout period
    \item Take the central 90\% confidence interval of all predictions for each customer
\end{enumerate}

\noindent This approach has not been implemented as part of this work but is generally implemented in the R package CLVTools \cite{CLVTools}. For more detailed information regarding this procedure, refer to this package. 

\subsubsection{Mini bootstrap / Ensemble}
As the name suggests, this method is a combination of several methods. Its idea has been introduced by \cite{57PI} and the implementation in the pnbd context follows their approach. 

\begin{enumerate}[label=Step \arabic*:, leftmargin=2cm]
    \item Fit one single pnbd model on the training period of all customers
    \item Receive parameter estimates and the respective covariance matrix
    \item From these estimates and the covariance matrix, simulate n draws of the parameters, assuming the parameters to be multivariate normal distributed
    \item Each draw of this parameter simulation is the basis for a new model
    \item Run the prediction for the holdout period with each of these new models
    \item Receive n values for the CET
    \item Take the central 90\% interval of the predictions for each customer
\end{enumerate}
Its great advantage compared to the bootstrap is that requires only a single model fit.

\subsubsection{Bayesian method}
This approach requires to fit the whole pnbd model with the Bayesian approach and then take the intervals from the posterior predictive distribution. As it would be out of scope for this work to re-estimate the model with the Bayesian approach, the existing implementation from the BTYDplus package \cite{BTYDplus} in R was used. Therefore, only a rough explanation how the Bayesian model fitting works will be given. For more detailed information, refer to the mentioned package.
\begin{enumerate}[label=Step \arabic*:, leftmargin=2cm]
    \item Estimate the pnbd model with the Bayesian approach: (BTYDplus package)
    \begin{itemize}
        \item No previous information about the parameter distribution is given
        \item Use the mcmc method to get the posterior parameter distribution
        \item From this distribution, use again the mcmc method to get the draws of the posterior predictive distribution
    \end{itemize}
    \item Take the central 90\% quantile of the posterior predictive CET distribution for each customer
\end{enumerate}

\subsubsection{Quantile regression}
As indicated before, the implementation of Quantile regression will be modified from the “original” approach but keeps the idea of direct interval estimation. To avoid introducing an optimization in the model fitting process, the optimization is 1. Conducted after the model fitting and 2. Broken down into to a grid search. In addition, for this method to work, it is necessary to have an old and a current cohort, each with a training and a holdout period. The reason for this issue will be more cleared in the following procedure and the explanation below.\\\\
This first part is conducted on the old cohort.
\begin{enumerate}[label=Step \arabic*:, leftmargin=2cm]
    \item Build a grid of potential parameter combinations \emph{1:M}
    \item Each parameter combination $m$ is the basis for a new model
    \item Predict the CET for each customer with every new model
    \item Each model $m$ will predict a certain amount of overestimations and underestimations, while it is the objective to find those combinations that yield 5\% and 95\% overpredictions. To assess how good each parameter combination $m$ performs, introduce a distance measure what serves as a loss function for the
    \begin{itemize}
        \item Upper bound parameters (only $\frac{\alpha}{2}$ of the true values should be \emph{above} the predictions)\\
        \begin{equation}
            loss\_upper_m = \left( \frac{1}{n} \sum_{i=1}^{n} \mathbbm{1} (y_{i} \ge est_{i}) \right) - \frac{\alpha}{2}
        \end{equation}
        \item Lower bound parameters (only $\frac{\alpha}{2}$\ of the true values should be \emph{below} the predictions)\\
        \begin{equation}
            loss\_lower_m = \left( \frac{1}{n} \sum_{i=1}^{n} \mathbbm{1} (y_{i} \le est_{i}) \right) - \frac{\alpha}{2}
        \end{equation}
        The first part of the equations measures exactly the coverage and then $\frac{\alpha}{2}$ is deducted.
    \end{itemize}
    \item Collect these calculated differences
    \item Select the parameter combinations $m_{1},m_{2}$ that yield the lowest absolute differences, one combination for the upper, one combination for the lower bound.
\end{enumerate}
It is apparent that the true values $y_{i}$ are required in this method in order to calculate the coverage of each combination. In reality, one does either not have these values (because they lay in future, and they shall be predicted) or they are known because they lay in the past. Then, there would be no point in predicting them. To overcome this issue, the previous procedure was carried out on an old data set and now one assumes that customer behavior for one firm will not change a lot over time. In this case, there is no reason to assume that the optimal parameter combinations that yield the desired quantiles would change. So, the first part is run on old data of the company to figure out the optimal parameter combination for each limit and then use those on the new, interesting data to construct prediction intervals.\\
This second part is conducted on the new cohort.
\begin{enumerate}[label=Step \arabic*:, leftmargin=2cm]
    \item Fit a model on the training period
    \item Make predictions an holdout (unknown) period
    \item Make also predictions using the 2 parameter combinations that were derived before as models
    \item These predictions are the upper and lower intervals limits
\end{enumerate}

A few notes on the first part of the procedure:
\begin{itemize}
    \item For setting up the grid, it is helpful to have some prior knowledge where parameters should be located approximately. This step was done manually, running several attempts by hand for a rough orientation and then giving several alternative values around. It turns out that, regardless of the dataset (and in the next chapter, regardless of the learning and holdout period lengths), nearly the same parameters are selected. This is a very convenient situation for the application in practice as the combinations do not need to be identified anymore and one can run the approach on a smaller grid.
    \item Many customers do not buy again after their initial purchase. Regardless, which parameter combination is selected, the model will never predict exactly 0, i.e. for the lower boundary, but something very close to 0. It makes sense to introduce a small tolerance and set those predictions to 0 which are reasonably close to 0 to give method a chance to perform well. This seems arbitrary but in practice, one could argue that a managerial decision for a customer will barely differ if CET = 0 or CET = 0.1 (what is the used tolerance). Also, one could argue that it is “unfair” against the other methods. Quantile regression is the only method that suffers from this issue and adding this tolerance increases at the same time the QR-interval’s width, so it comes at a cost. Doing this trade-off for other methods would not increase their performance.
\end{itemize}

\subsubsection{Conformal prediction}
As indicated before, only Split Conformal prediction will be implemented. The implementation follows in principle the steps from the general description but there are several modifications to be made.
\begin{enumerate}
    \item Heteroskedasticity of the outcomes: It appears that customers have a very different repurchasing behavior and might buy again 0 or 50 times. When the model is off by 3, say for the first customer ($y_{1}$ = 0), it predicts 3 and for the second customer ($y_{2}$ = 50) it predicts 53, the absolute delta would be equal, but the model would have done a bad job for the first customer and good job for the second customer. Assuming that model is equally good at all levels, it is reasonable to employ prediction intervals that are adaptive. Otherwise, the method would suffer from over coverage for small and under coverage for large values. A solution to that issue is given in \cite{1CP} \& \cite{9CP}, as they suggest scaling the residuals by their standard deviation, “studentization”. The approximate standard deviation for customers of one company can be approximated as a linear function of their CET level, retrieving $sd(CET_{i})$. This process is conducted on the old cohort can be summarized as
    \begin{enumerate}[label=Step \arabic*:, leftmargin=2cm]
        \item Fit pnbd-model on the training data
        \item Make predictions on the holdout period
        \item Get the absolute differences for each prediction to their true value
        \item Fit a linear model \(absolute\_difference(CET_{i}) \sim CET_{i}\)
    \end{enumerate}
    For every CET, there is now a reasonable scale.
    \item As it was the problem with Quantile regression before, Conformal prediction needs the true data as well, not only for the standard deviation but the actual method functionality. Again, one could make the assumption that customers’ behavior for one firm is approximately constant over time. The whole process of model fitting, and derivation of the quantile and standard deviation could therefore be conducted on the old cohort where the true transactions are known. Quantile and standard deviation are then forwarded to the current cohort.
    \item Compared to the standard procedure, described in the previous chapter, it is not necessary to use a validation set to get the quantiles. In the standard procedure, a model is fitted on training data and if the quantiles were taken from the predictions on these known training data, one would expect them to be too small because the model knows those data and would overfit compared to unknown data. This is not the case in the pnbd model because, in order to predict the CET of the new cohort, a new model is fitted on exactly those new customers. Therefore, it is expected that the quantiles fitted on the "known" training customers will be adequate for customers which are known to the new model. In other words, the model used to get the quantile knows its customers and the model on which the quantiles are applied, knows its customers as well, hence the quantiles should be adequate from this point of view.
\end{enumerate}
Assuming that the linear model for the standard deviation has been derived already, the whole process can be summarized:\\\\
For the old cohort
\begin{enumerate}[label=Step \arabic*:, leftmargin=2cm]
    \item Split the data set into a training and a test set (split customer wise)
    \item Train the pnbd-model on the training set
    \item Make point predictions on this training and test set
    \item Take the absolute deviations from the training set's true values and scale them (divide by the customers' respective estimated standard deviation from the previously fitted linear model)
    \item Take the desired x\%-quantile of the scaled residuals (formula)
    \item For each customer in the \textbf{test set}, rescale the quantile by multiplying with the individual linearly estimated standard deviation for each customer
    \item Add and subtract these individually scaled quantiles to/from the point predictions on the test set
    \item Evaluate the average coverage on the test set
\end{enumerate}
Theoretically, it is not necessary to split the old data set and make this test from step 5 to step 7, but it is reasonable to check if the quantile works at least with old data before transferring it to the new data.\\\\
For the new cohort
\begin{enumerate}[label=Step \arabic*:, leftmargin=2cm]
    \item Train the pnbd-model on the new training data
    \item Make point predictions for the holdout (unknown) period
    \item Rescale the quantile from the old cohort with the predicted standard deviation for each customer $i$: $quantile * sd(CET_{i})$
    \item Subtract and add the individually scaled quantile for each point prediction
\end{enumerate}


Important remark for Quantile regression and Conformal prediction: When assuming the temporal consistency of parameters (QR) and quantiles (CP), this is valid for both directions. This means that for long time customers, for whom there don't exist any previous cohort, the parameters/quantile taken from a new cohort should be valid as well when estimating their CET uncertainty. One might legitimately argue that their behavior is not expected to be the same like for new customers as we know that those 1. have been loyal for a long time and 2. might indeed physically die when considering they have been customers in e.g. a pharmacy for 40 years.

\subsubsection{Conceptual comparison of the methods}

\subsection{Data}
The introduced methods will be deployed on 4 data sets, containing transactions from customers of different retailing industries. As QR, CP and CR need an old cohort with training and prediction period to work, from all data sets an old cohort and a new cohort will be taken, each with fixed training and prediction periods that are equal for all methods. The learning periods for all data sets and cohorts are approximately 1 year. The holdout period for the gift and electronics data set are also 1 year for each cohort and for the multi-channel retailer and the apparel retailer, the holdout periods are 2 years. For more details regarding the data sets, see the tables below. The tables follow mainly the data descriptions in \cite{34L}.

Gift
\begin{table}[!ht]
    \centering
    \begin{adjustbox}{width=\textwidth}
    \begin{tabular}{|l|l|l|l|l|l|l|}
    \hline
         & Old learning & Old holdout & Total old & New learning & New holdout & Total new  \\ \hline
        Customers & - & - & 2124 & - & - & 2064  \\ \hline
        Transactions & 6103 & 7453 & 13556 & 6721 & 4509 & 11230  \\ \hline
        \makecell[l]{Available timeframe \\ and split in weeks} & 52 & 52 & 104 & 52 & 52 & 104  \\ \hline
        \makecell[l]{Average number of \\ purchases per customer} & 2.96 & 3.61 & 6.57 & 3.26 & 2.18 & 5.44  \\ \hline
        \makecell[l]{Standard deviation \\ of repeated purchases} & 4.71 & 5.91 & 7.34 & 4.13 & 7.16 & 6.92  \\ \hline
        Zero repeaters & 956 & 221 & 750 & 867 & 199 & 700  \\ \hline
        First entry date & - & - & 08.12.2002 & - & - & 08.12.2004  \\ \hline
        Last entry date & - & - & 15.12.2002 & - & - & 15.12.2004  \\ \hline
    \end{tabular}
    \end{adjustbox}
    \caption{Gift retailer data set}
    \label{gift data information}
\end{table}

el
\begin{table}[!ht]
    \centering
    \begin{adjustbox}{width=\textwidth}
    \begin{tabular}{|l|l|l|l|l|l|l|}
    \hline
        Metric & Old learning & Old holdout & Total old & New learning & New holdout & Total new  \\ \hline
        Customers & - & - & 728 & - & - & 4859  \\ \hline
        Transactions & 3206 & 3954 & 7160 & 20679 & 10282 & 30961  \\ \hline
        \makecell[l]{Available timeframe \\ and split in weeks} & 52 & 52 & 104 & 52 & 52 & 104  \\ \hline
        \makecell[l]{Average number of \\ purchases per customer} & 0.66 & 0.81 & 1.47 & 4.26 & 2.12 & 6.37  \\ \hline
        \makecell[l]{Standard deviation \\ of repeated purchases} & 4.18 & 5.06 & 5.92 & 4.44 & 5.73 & 6.1  \\ \hline
        Zero repeaters & 183 & 58 & 138 & 1264 & 208 & 1058  \\ \hline
        First entry date & - & - & 01.01.2000 & -& -& 01.01.2002  \\ \hline
        Last entry date & -& -& 31.03.2000 & -& -& 30.03.2002  \\ \hline
    \end{tabular}
    \end{adjustbox}
    \caption{Electronics retailer data set}
    \label{el data information}
\end{table}

multi
\begin{table}[!ht]
    \centering
    \begin{adjustbox}{width=\textwidth}
    \begin{tabular}{|l|l|l|l|l|l|l|}
    \hline
        Metric & Old learning & Old holdout & Total old & New learning & New holdout & Total new  \\ \hline
        Customers & -& - & 3644 & - & - & 3885  \\ \hline
        Transactions & 7365 & 2143 & 9508 & 7632 & 839 & 8471  \\ \hline
        \makecell[l]{Available timeframe \\ and split in weeks} & 52 & 104 & 156 & 60 & 104 & 164  \\ \hline
        \makecell[l]{Average number of \\ purchases per customer} & 1.9 & 0.55 & 2.45 & 1.96 & 0.22 & 2.18  \\ \hline
        \makecell[l]{Standard deviation \\ of repeated purchases} & 1.92 & 2.6 & 2.44 & 1.97 & 1.75 & 2.23  \\ \hline
        Zero repeaters & 2020 & 212 & 1823 & 2223 & 138 & 2103  \\ \hline
        First entry date & - & - & 01.12.2005 & - & - & 01.12.2008  \\ \hline
        Last entry date & - & - & 31.12.2005 & - & - & 31.12.2008  \\ \hline
    \end{tabular}
    \end{adjustbox}
    \caption{Multi-channel retailer data set}
    \label{multi data information}
\end{table}

Apparel
\begin{table}[!ht]
    \centering
    \begin{adjustbox}{width=\textwidth}
    \begin{tabular}{|l|l|l|l|l|l|l|}
    \hline
        & Old learning & Old holdout & Total old & New learning & New holdout & Total new  \\ \hline
        Customers & - & - & 814 & - & - & 2836  \\ \hline
        Transactions & 1725 & 3739 & 5464 & 5561 & 6789 & 12350  \\ \hline
        \makecell[l]{Available timeframe \\ and split in weeks} & 52 & 104 & 156 & 52 & 104 & 156  \\ \hline
        \makecell[l]{Average number of \\ purchases per customer} & 0.61 & 1.32 & 1.93 & 1.96 & 2.39 & 4.35  \\ \hline
        \makecell[l]{Standard deviation \\ of repeated purchases} & 1.62 & 2.75 & 3.76 & 1.58 & 2.82 & 3.66  \\ \hline
        Zero repeaters & 383 & 150 & 256 & 1545 & 544 & 904  \\ \hline
        First entry date & - & - & 01.01.2000 & - & - & 01.01.2003  \\ \hline
        Last entry date & - & - & 15.01.2000 & - & - & 15.01.2003  \\ \hline
    \end{tabular}
    \end{adjustbox}
    \caption{Apparel retailer data set}
    \label{apparel data information}
\end{table}

\begin{table}[]
\centering
    \setlength{\tabcolsep}{2pt} % Adjust space between columns
    \renewcommand{\arraystretch}{1.5} % Adjust row height
    \begin{adjustbox}{width=\textwidth}
        \begin{tabular}{|>{\raggedright\arraybackslash}m{1.5cm}>{\raggedright\arraybackslash}m{1cm}|>{\raggedright\arraybackslash}m{2cm}|>{\raggedright\arraybackslash}m{3cm}|>{\raggedright\arraybackslash}m{2cm}|>{\raggedright\arraybackslash}m{2.5cm}|>{\raggedright\arraybackslash}m{2cm}|>{\raggedright\arraybackslash}m{2cm}|}
        \hline
         \multicolumn{2}{|m{4cm}|}{Method} & Focused uncertainty & Assumptions & True values needed & Computational effort & Approach complexity & Frequentist approach \\ \hline
        \multicolumn{2}{|m{4cm}|}{Bootstrap} & Epistemic & In this context none & No & Medium & Low & Yes \\ \hline
        \multicolumn{2}{|m{4cm}|}{Ensemble} & Epistemic & Normal distribution of re-sampled parameters & No & Low & Low & Yes \\ \hline
        \multicolumn{2}{|m{4cm}|}{Bayesian} & Epistemic, Aleatory & None but knowledge about prior parameter distr. can enhance results & No & High & High & No \\ \hline
        \multicolumn{2}{|m{4cm}|}{Quantile regression} & Epistemic, Aleatory & None & Yes & Depends on prior knowledge on parameters & Medium & Yes \\ \hline
        \multicolumn{1}{|m{2cm}|}{\multirow{2}{*}{\begin{tabular}[c]{@{}m{4cm}@{}}Conformal\\ prediction\end{tabular}}} & CP & Epistemic, Aleatory & Exchangeability & Yes & Low & Medium & Yes \\ \cline{2-8} 
        \multicolumn{1}{|m{2cm}|}{} & CR & Epistemic, Aleatory & Exchangeability & Yes & High & High & Yes \\ \hline
    \end{tabular}
    \end{adjustbox}
\end{table}


\subsection{Results}

\subsubsection{Methods' performances and benchmarking the bootstrap approach}
In the course of this work, all  methods are applied to all datasets and evaluated with the measures that were introduced above. The following table contains these results, which shall be subject to some deeper analyses.

\paragraph{General performance}
\begin{landscape}
    \begin{table}[!ht]
    \centering
    \setlength{\tabcolsep}{3pt} % Adjust the horizontal spacing between columns
    \renewcommand{\arraystretch}{1.2} % Adjust the vertical spacing between rows
        \begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|l|l|}
            \hline
            \makecell[l]{\textbf{Method}} & \makecell[l]{\textbf{Data}} & \makecell[l]{\textbf{PICP}} & \makecell[l]{\textbf{ACE}} & \makecell[l]{\textbf{PICPW}} & \makecell[l]{\textbf{PIARW}} & \makecell[l]{\textbf{PIARWW}} & \makecell[l]{\textbf{MSIS}} & \makecell[l]{\textbf{SWR}} & \makecell[l]{\textbf{Upper}\\ \textbf{coverage}} & \makecell[l]{\textbf{Lower}\\ \textbf{coverage}} & \makecell[l]{\textbf{Time in sec}\\ \textbf{(abs)}} & \makecell[l]{\textbf{Time (rel)}} \\
            \hline
            \multirow{4}{*}{BS} & gift & 0.0141 & 0.8859 & 0.0375 & 0.2682 & 0.2816 & 37.3243 & 0.0524 & 0.7602 & 0.2539 & 1228.391 & 74.5412 \\
            & electronics & 0.0027 & 0.8973 & 0.0094 & 0.2176 & 0.1821 & 93.2005 & 0.0123 & 0.8133 & 0.1893 & 725.2538 & 44.0099 \\
            & multi & - & - & - & - & - & - & - & - & - & - & - \\
            & apparel & 0.0201 & 0.8799 & 0.0333 & 0.1293 & 0.1102 & 21.7756 & 0.1555 & 0.6410 & 0.3791 & 993.5866 & 60.2928 \\
            \hline        
            \multirow{4}{*}{EN} & gift & 0.0107 & 0.8893 & 0.0285 & 0.3116 & 0.3083 & 36.0972 & 0.0342 & 0.7602 & 0.2505 & 49.4184 & 2.9988 \\
            & electronics & 0.0037 & 0.8963 & 0.0126 & 0.1749 & 0.1535 & 93.6494 & 0.0212 & 0.8142 & 0.1895 & 80.8995 & 4.9091 \\
            & multi & 0.0000 & 0.9000 & 0.0000 & 0.5804 & 0.4701 & 27.0970 & 0.0000 & 0.9194 & 0.0806 & 279.6304 & 16.9685 \\
            & apparel & 0.0240 & 0.8760 & 0.0368 & 0.1501 & 0.1171 & 21.5615 & 0.1597 & 0.6410 & 0.3829 & 64.5908 & 3.9195 \\
            \hline
            \multirow{4}{*}{BA} & gift & 0.9830 & -0.0830 & 0.8137 & 10.9538 & 9.4455 & 13.0295 & 0.0897 & 0.983 & 1.0000 & 746.6716 & 45.3095 \\
            & electronics & 0.8837 & 0.0163 & 0.4920 & 4.5601 & 4.9435 & 75.8657 & 0.1938 & 0.8841 & 0.9996 & 87.1369 & 5.2876 \\
            & multi & 0.9761 & -0.0761 & 0.5909 & 9.5724 & 8.3749 & 17.2195 & 0.1020 & 0.9761 & 1.0000 & 8392.8783 & 509.2968 \\
            & apparel & 0.9644 & -0.0644 & 0.8582 & 4.8213 & 4.0899 & 6.8836 & 0.2000 & 0.969 & 0.9954 & 146.4306 & 8.8857 \\
            \hline
            \multirow{4}{*}{QR} & gift & 0.9520 & -0.0520 & 0.6003 & 8.0923 & 6.5505 & 12.1046 & 0.1176 & 0.9520 & 1.0000 & 321.1329 & 19.4870 \\
            & electronics & 0.9337 & -0.0337 & 0.4799 & 26.6989 & 19.5407 & 44.7260 & 0.0350 & 0.9341 & 0.9996 & 265.6469 & 16.1200 \\
            & multi & 0.9843 & -0.0843 & 0.6455 & 18.2627 & 14.4328 & 21.1570 & 0.0539 & 0.9843 & 1.0000 & 551.7171 & 33.4793 \\
            & apparel & 0.9076 & -0.0076 & 0.6088 & 3.7018 & 2.7961 & 6.7810 & 0.2452 & 0.9087 & 0.9989 & 264.0006 & 16.0201 \\
            \hline
            \multirow{4}{*}{CP} & gift & 0.9549 & -0.0549 & 0.6171 & 6.0575 & 5.2245 & 11.702 & 0.1576 & 0.9549 & 1.0000 & 17.3019 & 1.0499 \\
            & electronics & 0.9333 & -0.0333 & 0.5290 & 14.6561 & 11.2136 & 49.3349 & 0.0637 & 0.9333 & 1.0000 & 16.4793 & 1.0000 \\
            & multi & 0.9441 & -0.0441 & 0.3023 & 10.1164 & 8.7027 & 16.7734 & 0.0933 & 0.9441 & 1.0000 & 410.5946 & 24.9157 \\
            & apparel & 0.8798 & 0.0202 & 0.6336 & 2.7881 & 2.3939 & 7.5872 & 0.3155 & 0.8946 & 0.9852 & 11.0430 & 0.6701 \\
            \hline
            \multirow{4}{*}{CR} & gift & 0.9549 & -0.0549 & 0.6171 & 6.0952 & 5.2560 & 11.7080 & 0.1567 & 0.9549 & 1.0000 & 224.1160 & 13.5998 \\
            & electronics & 0.8829 & 0.0171 & 0.4107 & 13.8512 & 10.6115 & 49.9486 & 0.0637 & 0.8829 & 1.0000 & 187.4498 & 11.3748 \\
            & multi & 0.9441 & -0.0441 & 0.3023 & 10.1294 & 8.7137 & 16.7711 & 0.0932 & 0.9441 & 1.0000 & 3593.856 & 218.0824 \\
            & apparel & 0.8801 & 0.0199 & 0.6338 & 2.8360 & 2.4368 & 7.5273 & 0.3103 & 0.8946 & 0.9855 & 60.9358 & 3.6977 \\
            \hline
        \end{tabular}
        \caption{Results table with grouped method rows using multirow.}
\end{table}
\end{landscape}
\noindent The results table from the previous page have been summarized by every method by averaging over the respective results and a ranking of these averages has been produced. The following insights in the method performance are to be noted.

\begin{table}[h!]
    \centering
     % Shift the table 1cm to the left
    \setlength{\tabcolsep}{4pt} % Adjust column separation
    \renewcommand{\arraystretch}{1.2} % Adjust row separation
    \begin{adjustbox}{width=\textwidth}
    \begin{tabular}{|p{1.4cm}|p{1cm}|p{1.1cm}|p{1.4cm}|p{1.4cm}|p{1.8cm}|p{1.2cm}|p{1cm}|p{1.2cm}|p{1.2cm}|p{1.4cm}|}
        \hline
        \makecell[l]{\textbf{Method}} & \makecell[l]{\textbf{PICP}} & \makecell[l]{\textbf{ACE}} & \makecell[l]{\textbf{PICPW}} & \makecell[l]{\textbf{PIARW}} & \makecell[l]{\textbf{PIARWW}} & \makecell[l]{\textbf{MSIS}} & \makecell[l]{\textbf{SWR}} & \makecell[l]{\textbf{Upper}\\ \textbf{cov.}} & \makecell[l]{\textbf{Lower}\\ \textbf{cov.}} & \makecell[l]{\textbf{Time [s]}\\ \textbf{(abs)}}\\
        \hline
        BS & 0.0123 & 0.8877 & 0.0267 & 0.2050 & 0.1913 & 50.7668 & 0.0734 & 0.7382 & 0.2741 & 982.4105 \\
        \hline
        EN & 0.0096 & 0.8904 & 0.0195 & 0.3043 & 0.2623 & 44.6013 & 0.0538 & 0.7837 & 0.2259 & 118.6348 \\
        \hline
        BA & 0.9518 & -0.0518 & 0.6887 & 7.4769 & 6.7135 & 28.2496 & 0.1463 & 0.9531 & 0.9988 & 2343.2794 \\
        \hline
        QR & 0.9444 & -0.0444 & 0.5836 & 14.1890 & 10.8300 & 21.1922 & 0.1130 & 0.9448 & 0.9996 & 350.6244 \\
        \hline
        CP & 0.9280 & -0.0280 & 0.5205 & 8.4045 & 6.8837 & 21.3494 & 0.1575 & 0.9317 & 0.9963 & 113.8547 \\
        \hline
        CR & 0.9155 & -0.0155 & 0.4910 & 8.2280 & 6.7545 & 21.4888 & 0.1560 & 0.9191 & 0.9964 & 1016.5894 \\
        \hline
        \end{tabular}
    \end{adjustbox}{}    
    \caption{Averages table}
\end{table}

\begin{table}[h!]
    \centering
     % Shift the table 1cm to the left
    \setlength{\tabcolsep}{4pt} % Adjust column separation
    \renewcommand{\arraystretch}{1.2} % Adjust row separation
    \begin{adjustbox}{width=\textwidth}
    \begin{tabular}{|p{1.4cm}|p{1cm}|p{1.1cm}|p{1.4cm}|p{1.4cm}|p{1.8cm}|p{1.2cm}|p{1cm}|p{1.2cm}|p{1.2cm}|p{1.4cm}|}
        \hline
        \makecell[l]{\textbf{Method}} & \makecell[l]{\textbf{PICP}} & \makecell[l]{\textbf{ACE}} & \makecell[l]{\textbf{PICPW}} & \makecell[l]{\textbf{PIARW}} & \makecell[l]{\textbf{PIARWW}} & \makecell[l]{\textbf{MSIS}} & \makecell[l]{\textbf{SWR}} & \makecell[l]{\textbf{Upper}\\ \textbf{cov.}} & \makecell[l]{\textbf{Lower}\\ \textbf{cov.}} & \makecell[l]{\textbf{Time [s]}\\ \textbf{(abs)}}\\ \hline
        BS & 5 & 5 & 5 & 1 & 1 & 6 & 5 & 6 & 5 & 4  \\ \hline
        EN & 6 & 6 & 6 & 2 & 2 & 5 & 6 & 5 & 6 & 2  \\ \hline
        BA & 1 & 4 & 1 & 3 & 3 & 4 & 3 & 1 & 2 & 6  \\ \hline
        QR & 2 & 3 & 2 & 6 & 6 & 1 & 4 & 2 & 1 & 3  \\ \hline
        CP & 3 & 2 & 3 & 5 & 5 & 2 & 1 & 3 & 4 & 1  \\ \hline
        CR & 4 & 1 & 4 & 4 & 4 & 3 & 2 & 4 & 3 & 5  \\ \hline
        \end{tabular}
    \end{adjustbox}{}    
    \caption{Ranking table}
\end{table}

\noindent Major findings
\begin{enumerate}
    \item The 2 bootstrap-based methods deliver consistently under coverage across data sets (BS, EN) while the 4 other methods deliver roughly the desired or even over coverage (BA, CP, CR, QR).
    \item The widths of the 2 underperforming methods are significantly lower than the widths of the other 4 methods.
    \item There is not trade-off method that combines both strengths and finds a compromise. This is visualized in graphics…
    \item CP and EN are significantly faster than all other methods, BA is the most time consuming.
\end{enumerate}

\noindent Other findings
\begin{enumerate}
    \item When over weighing more important customers, generally decreases the the coverage, meaning that all methods perform better with important customers. The 4 satisfyingly covering methods range roughly between 50\% and 70\% with BA leading with 68.9\% over the next closest, QR with 58.4\%. The 2 bootstrap-based methods give relatively signifantly better results than without the weighing but are both still below 3\%.
    \item Regarding the width, EN and BS have the by far lowest width with the respect to the prediction, followed by BA, CR and CP, serving a middle way. QR has in 2 data sets comparable values to these 3 methods but in the electronics and the multichannel case, it develops very far intervals, resulting in a very high average. One can infer for that the performance of QR has higher variability and dependence of the data set.
    \item Intervals for all methods shrink in relation to CET. This can be mostly explained by not scaling with excessively small numbers.
    \item The combined assessment of sharpness and width sees BS and EN on the lower rank, due to the penalization of non-coverages. The rest of the methods have roughly the same performance regarding MSIS with a slight disadvantage for BA.
    \item The same picture can be seen in SWR. The 4 reliable methods have around 2-3x more coverage per width than BS and EN.
    \item CP and CR hit the desired 90\% the most accurate. (least over or under covering)
    \item BS and EN have, scaled to their actual performance, a high variability (coefficient of variation) in reliability measures across data sets. For sharpness measures, all methods have comparable variability across data sets, see variation table (table \ref{Variation table} in the appendix which contains the coefficient of variation across datasets for each method and measure.
\end{enumerate}

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{Plots/PICP and PIARW by Method and Data Set.png}
    \caption{Distribution of amount of transactions}
    \label{fig:enter-label}
\end{figure}

\noindent One can clearly observe the two different types of methods, the bootstrap-based on the left with low coverage and small intervals on the left and the other methods on the right, delivering the desired coverage at the cost of wider intervals. What is also interesting to note is that data sets cause either wider or shorter intervals across CR, CP and QR. This means that the methods realize model uncertainty or inaccuracy and react to it, as the widen the intervals to consistently deliver the desired coverage (what is what they are optimizing for). This can also be observed for EN and BS but in another order, surprisingly.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{Plots/90 PIs apparel_results.png}
    \caption{Distribution of amount of transactions}
    \label{fig:enter-label}
\end{figure}

\noindent This graph shows the concrete lengths of prediction intervals for concrete customers and the true number of transactions exemplary for the apparel data set (see in the appendix the graphs for the other data sets). Again, one can observe the small ranges that are covered by EN and BS and the wide spread of true observations that show how unlikely it is for these methods to cover a true point. The other methods capture the uncertainty appropriately and (in this case at least) cover all true points. For this data set and selected range of CET, BA has the longest intervals, but this is not representative for the rest of the data sets and ranges of CET as the overview table shows and as can be seen below.\\
\\
Summary\\
There are two types of methods, those width wide intervals and appropriate coverage and those with narrow intervals and low coverage. In combined measures of reliability and sharpness, usually the methods with high coverage outperform the other two. These domains of method expertise are visualized in the radar chart below.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{Plots/Radar chart.png}
    \caption{Distribution of amount of transactions}
    \label{fig:enter-label}
\end{figure}

\paragraph{PICP and width across CET levels} \mbox{}\\
In the general insights section, it was already mentioned that methods might perform differently at different levels of CET. This shall be examined more in-depths.

\subparagraph{PICP}\mbox{}\\
As the coverage develops across different CET levels, this development is not quite the same for different data sets, therefore see below this development for all data sets. The following graphs smooth the coverage across CET by the help of a kernel with normal distribution.\\
Across all data sets, BA has the highest coverage what is not surprising because is has the highest over coverage. EN and BS have the lowest coverage, close to 0, across all levels. The second note to make is generally a downward tendency that is approximately true for all methods on all data sets. This means that the intervals are less likely to include the true value if it is high. After the analysis of PICPW, this comes with no surprise but poses an issue as it is exactly this segment that marketers are interested in. The high coverage rate at lower values comes from many true values being 0 and usually intervals are covering 0 either by nature or by the small tolerance that was given in case of QR. The lower coverage on high values is visible across methods what leads to the conclusion that the model might be less accurate in this area. Indeed, the relative error \(\left(\frac{|CET_{i}-y_{i}|}{CET_{i}}\right)\) for higher true values is a lot higher than for smaller values (see figure \ref{Relative model error} in the appendix). In the methods, this behavior is not covered and hence explains as well the downward trend.

\begin{figure}[htp]
\centering

\begin{subfigure}{0.49\columnwidth}
\centering
\includegraphics[width=\textwidth]{Plots/Coverage development with CET for gift_results.png}
\caption{}
\label{fig:time1}
\end{subfigure}\hfill
\begin{subfigure}{0.49\columnwidth}
\centering
\includegraphics[width=\textwidth]{Plots/Coverage development with CET for el_results.png}
\caption{}
\label{fig:time2}
\end{subfigure}

\medskip

\begin{subfigure}{0.49\columnwidth}
\centering
\includegraphics[width=\textwidth]{Plots/Coverage development with CET for multi_results.png}
\caption{}
\label{fig:time3}
\end{subfigure}\hfill
\begin{subfigure}{0.49\columnwidth}
\centering
\includegraphics[width=\textwidth]{Plots/Coverage development with CET for apparel_results.png}
\caption{}
\label{fig:time4}
\end{subfigure}

\medskip

\begin{subfigure}{0.49\columnwidth}
\centering
\includegraphics[width=\textwidth]{Plots/Legend 1.png}
\caption{}
\label{fig:time5}
\end{subfigure}

\caption{Boxplots}
\label{fig:time}

\end{figure}



\subparagraph{Width} \mbox{}\\
Considering PICP across CET levels, it must be seen in connection with the respective width development. The situation across data sets is very heterogeneous but also here, there is generally a downward trend visible, while either BA or QR delivering the widest intervals and BS and EN delivering across all levels intervals with a length of nearly 0. The downward trend is not as dominant as for PICP but still visible. A lot of this phenomenon can be explained by dividing “normal”-sized intervals at lower levels by very small predictions, delivering relatively wider intervals. The key insight therefore is that a decreasing PICP with increasing customer value is first and foremost not the responsibility of relatively narrowing PIs. Nevertheless, it is arguable that more sophisticated methods should realize the differentiated model performance and adapt their PIs accordingly.

\begin{figure}[h]
\centering

\begin{subfigure}{0.49\columnwidth}
\centering
\includegraphics[width=\textwidth]{Plots/Width development with CET gift_results.png}
\caption{}
\label{fig:time1}
\end{subfigure}\hfill
\begin{subfigure}{0.49\columnwidth}
\centering
\includegraphics[width=\textwidth]{Plots/Width development with CET el_results.png}
\caption{}
\label{fig:time2}
\end{subfigure}

\medskip

\begin{subfigure}{0.49\columnwidth}
\centering
\includegraphics[width=\textwidth]{Plots/Width development with CET multi_results.png}
\caption{}
\label{fig:time3}
\end{subfigure}\hfill
\begin{subfigure}{0.49\columnwidth}
\centering
\includegraphics[width=\textwidth]{Plots/Width development with CET apparel_results.png}
\caption{}
\label{fig:time4}
\end{subfigure}

\medskip

\begin{subfigure}{0.49\columnwidth}
\centering
\includegraphics[width=\textwidth]{Plots/Legend 1.png}
\caption{}
\label{fig:time5}
\end{subfigure}

\caption{Boxplots}
\label{fig:time}

\end{figure}

\subparagraph{Summary} \hbox{}\\

\begin{table}[h!]
    \centering
    % Shift the table 1cm to the left
    \setlength{\tabcolsep}{4pt} % Adjust column separation
    \renewcommand{\arraystretch}{1.2} % Adjust row separation
    \begin{adjustbox}{width=\textwidth}
    \begin{tabular}{|p{1.5cm}|p{1cm}|p{1.3cm}|p{1.3cm}|p{1.3cm}|p{1.3cm}|p{1.2cm}|p{1.2cm}|}
    \hline
        {\makecell[l]{Method}} & 
        \rotatebox{90}{\makecell[l]{Adequate \\ coverage}} & 
        \rotatebox{90}{\makecell[l]{Consistent coverage \\ across outcomes \\ (CET, PTS)}} & 
        \rotatebox{90}{\makecell[l]{Consistent coverage \\ across outcome \\ levels}} & 
        \rotatebox{90}{\makecell[l]{Consistent \\ performance \\ across data sets}} & 
        \rotatebox{90}{\makecell[l]{Works in all \\ examined datasets}} & 
        \rotatebox{90}{\makecell[l]{Usefulness}} & 
        \rotatebox{90}{\makecell[l]{Computational \\ intensity}} \\ \hline
        Bootstrap & No & Not tested & No & Yes & No & Low & High  \\ \hline
        Ensemble & No & Yes & No & Yes & Yes & Low & Low  \\ \hline
        Bayesian & Yes & Not tested & No & Yes & Yes & Medium & High  \\ \hline
        CP & Yes & Yes & No & Yes & Yes & Medium & Low  \\ \hline
        CR & Yes & Yes & No & Yes & Yes & Medium & Medium  \\ \hline
        Quantile regression & Yes & often under coverage for PTS \footnotemark{} & No & Coverage yes, but not width & Yes & Medium & Medium  \\ \hline
    \end{tabular}
    \end{adjustbox}    
    \caption{Results summary table}
\end{table}
\footnotetext{Detailed results for PTS can be made available on request.}

\subsubsection{Performance over varying training and prediction periods} \hbox{}\\
Motivation\\
The analysis so far has held constant the learning and prediction period for each data set and examined how different performance measures vary across methods and CET levels. This chapter will deal with the cases when the periods for learning and prediction are varied and assess how the overall coverage is influenced. This scrutiny is mainly motivated by the methods QR, CP and CR which use old data from previous periods. Those data exist in the case of this work and the used data sets, but in reality, a lack of past data is not unrealistic and might cause problems. I.e. the data could be biased so that inappropriate quantiles (CR and CP) are learnt, or the wrong parameters are selected (QR). Another issue is that the model has difficulty to be fitted due to the lack of data and inherently loses predictive power. As this topic is not the core of this work, it will be held concise and concentrate on the most central insights.\\
Implementation\\
Due to computational reasons, the analysis will be limited to the electronics and gift data sets and the bootstrap based methods will be left out because only coverage is assessed here and a loss in this area is more relevant to BA, QR, CP and CR. For both data sets, around 50 combinations of learning and prediction times for the old and the new cohorts are used. For each period combination, a model is fitted, predictions are made, and prediction intervals are derived and assessed regarding coverage. The following graph resulted from the results of this procedure which are available on request.

\begin{figure}[h!]
    \centering
    \includegraphics[width=\linewidth]{Plots/Periods.png}
    \caption{Distribution of amount of transactions}
    \label{fig:enter-label}
\end{figure}

BA and QR were, regarding coverage, not affected by changing period lengths or data set and deliver constantly around the desired 90\% quantile. In contrast, the performance of the conformal prediction implementations suffer for the gift data set a lot while delivering decent results for the electronics data set. Looking directly gives several insights but no definite explanation for the underperformance and especially not for the difference between the two data sets. Typically, low values resulted from short learning or prediction periods for the old cohort, what makes intuitively sense as it is here where the quantiles are derived. On the other hand, one can also observe cases in which all periods, for both cohorts have been low which performed decently. It suggests that when both cohorts have been treated equally in terms of model fitting and quantile forming conditions, the system works properly.
The motivation for this chapter was to give an intuition how stable the methods are and if managers can apply them safely. QA and QR seem to work stable, regardless of data set or learning or prediction time. The Conformal prediction-based methods seem to suffer on some datasets but work well on other ones. The reasoning behind it cannot be clarified absolutely but tendencies are observable. It will be necessary to use more runs and data sets to get deeper results for all methods but one should be especially careful to use CP or CR with short periods.

\subsubsection{Application in marketing}
In this final chapter, the usefulness of the results beyond the assessment of the model uncertainty will be discussed. In particular, it shall be examined if and how prediction intervals can help to identify valuable customers. There have been numerous studies about how to identify valuable customers, amongst others the so-called RFM analysis where Recency, Frequency and the Monetary value of transactions. In this study the focus is laid on how the additional information about uncertainty can improve a rating of customers that is only done with the point predictor which is taken as a benchmark. \\
Implementation\\
\begin{enumerate}
    \item Train the model and make predictions for each data set using the above mentioned learning and holdout periods
    \item Calculate prediction intervals with all methods for all data sets
    \item Apply different strategies / metrics to the outcomes and rate customers with them
    \item Order the customers according to these metrics
    \item Pick the top x\%\footnotemark \hspace{2pt} of the customers according to the respective metric and compare the selected customers with the actual top performing customers.
    \footnotetext{The target is to pick the 10\% customers with the highest true value.This is not always realizable because the number of purchases made has few possible outcomes, so that after the last customer at 10\% has 4 purchases and the next one, outside these 10\%, also has 4 purchases. It would not be reasonable to include one and exclude the other, so that 10\% cannot be perfectly achieved and the closest number is taken.}
\end{enumerate}

\noindent For this, the following metrics have been implemented.
\begin{enumerate}[label=Metric \arabic*:, leftmargin=2cm]
    \item \textbf{Benchmark} hpp: Highest Point Predictor (CET)
    \item hul: Highest Upper Limit
    \item hiw: Highest interval width
    \item huu: Highest Upwards Uncertainty (the difference between the CET and the upper interval limit)
    \item htp: Highest Three-Point Estimate: The lower limit, the CET and the upper limit are weighted equally \(\left(\frac{1}{3}* (LL_{i} + CET_{i} + UL_{i})\right)\)
    \item csw: The CET is squared and divided by the interval width \(\left( \frac{CET_{i}^2}{UL_{i}-LL{i}}\right)\)
    \item ssq: The CET is squared and divided by the square root of the width \(\left(\frac{CET_{i}^2}{\sqrt{UL_{i}-LL_{i}}}\right)\)
\end{enumerate}

\noindent The following table shows the results for each data set, method and metric.\\
Note: max\_rel states how much \% of the customers are considered \emph{Top-customers}, which should be ideally close to 10\%. max\_abs is the actual number of \emph{Top-customers} in the respective dataset. All values for the previously introduced methods tell how much percent of respective \emph{Top-customers} have been identified by the metric. Example: For the gift data set, 6\% of all customers were identified as \emph{Top-customers} and hpp managed to identify 22.61\% of those 6\%.

\begin{table}[!ht]
    \centering
    \begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|}
    \hline
        Data & Method & max\_rel & max\_abs & hpp & hul & hiw & huu & htp & csw & ssq  \\ \hline
        gift & BS & 0.06 & 127 & 0.2261 & 0.2342 & 0.2422 & 0.2342 & 0.2342 & 0.105 & 0.2342  \\ \hline
        gift & EN & 0.06 & 127 & 0.2261 & 0.2342 & 0.2342 & 0.2422 & 0.2342 & 0.2099 & 0.2342  \\ \hline
        gift & BA & 0.06 & 127 & 0.2261 & 0.2261 & 0.2261 & 0.1615 & 0.218 & 0.2019 & 0.2099  \\ \hline
        gift & QR & 0.06 & 127 & 0.2261 & 0.2584 & 0.2584 & 0.1857 & 0.2422 & 0.2342 & 0.2342  \\ \hline
        gift & CP & 0.06 & 127 & 0.2261 & 0.2261 & 0.2261 & 0.2261 & 0.2261 & 0.2261 & 0.2261  \\ \hline
        el & BS & 0.22 & 1088 & 0.2414 & 0.2423 & 0.2161 & 0.2198 & 0.2423 & 0.2357 & 0.2367  \\ \hline
        el & EN & 0.22 & 1088 & 0.2414 & 0.2414 & 0.217 & 0.2217 & 0.2414 & 0.2414 & 0.2395  \\ \hline
        el & BA & 0.22 & 1088 & 0.2414 & 0.2554 & 0.2554 & 0.2292 & 0.2367 & 0.4041 & 0.4041  \\ \hline
        el & QR & 0.22 & 1088 & 0.2414 & 0.203 & 0.203 & 0.102 & 0.2273 & 0.2414 & 0.2423  \\ \hline
        el & CP & 0.22 & 1088 & 0.2414 & 0.2414 & 0.2414 & 0.2414 & 0.2414 & 0.2414 & 0.2414  \\ \hline
        multi & BS & - & - & - & - & - & - & - & - & -  \\ \hline
        multi & EN & 0.08 & 313 & 0.296 & 0.296 & 0.2156 & 0.2349 & 0.296 & 0.296 & 0.296  \\ \hline
        multi & BA & 0.08 & 313 & 0.296 & 0.3314 & 0.3314 & 0.2542 & 0.2928 & 0.0869 & 0.0869  \\ \hline
        multi & QR & 0.08 & 313 & 0.296 & 0.251 & 0.251 & 0.2156 & 0.2928 & 0.296 & 0.296  \\ \hline
        multi & CP & 0.08 & 313 & 0.296 & 0.296 & 0.296 & 0.296 & 0.296 & 0.296 & 0.296  \\ \hline
        apparel & BS & 0.12 & 332 & 0.3996 & 0.3996 & 0.332 & 0.3673 & 0.4026 & 0.3526 & 0.3967  \\ \hline
        apparel & EN & 0.12 & 332 & 0.3996 & 0.3996 & 0.2909 & 0.2909 & 0.3996 & 0.3702 & 0.3967  \\ \hline
        apparel & BA & 0.12 & 332 & 0.3996 & 0.3937 & 0.3937 & 0.3702 & 0.3996 & 0.3996 & 0.4026  \\ \hline
        apparel & QR & 0.12 & 332 & 0.3996 & 0.3996 & 0.3996 & 0.0441 & 0.3996 & 0.4026 & 0.4026  \\ \hline
        apparel & CP & 0.12 & 332 & 0.3996 & 0.3996 & 0.3996 & 0.3996 & 0.3996 & 0.3996 & 0.3996  \\ \hline
    \end{tabular}
\end{table}

\noindent For more visibility, the results are summarized as follows:

\begin{table}[!ht]
    \centering
    \begin{tabular}{|l|l|l|l|l|l|l|}
    \hline
        Metric & hul & hiw & huu & htp & csw & ssq  \\ \hline
        Better or equal & 0.8421 & 0.5789 & 0.3158 & 0.7368 & 0.6316 & 0.6842  \\ \hline
        Better & 0.3158 & 0.2632 & 0.1053 & 0.2632 & 0.1579 & 0.3684  \\ \hline
        Worse & 0.1579 & 0.4211 & 0.6842 & 0.2632 & 0.3684 & 0.3158  \\ \hline
        Mean advantage (rel) & 0.0037 & -0.043 & -0.1627 & 0.001 & -0.0483 & -0.001  \\ \hline
        Sd of advantages & 0.0185 & 0.038 & 0.0843 & 0.0063 & 0.0686 & 0.0626  \\ \hline
    \end{tabular}
\end{table}

\noindent The results are throughout all metrics, and the benchmark not very satisfying. The best metrics usually identify between 20\% and 40\% of the top-customers. These best metrics are in most of the cases the benchmark (highest point predictor hpp), the highest upper limit hul, and the three-point estimate htp. The latter 2 metrics beat the benchmark in 32\% and 26\% of the cases but by very little so that speaking about a true advantage would be misleading. The results from csw and ssq vary more and beat the benchmark in 1 case significantly but also lose significantly in another one without any recognizable pattern. It is noteworthy that all metrics, except the previously mentioned 2 cases, perform equally good (bad) across methods what was not to expect, regarding the severe differences in coverage power of the methods. \footnotemark{}
\footnotetext{This can be explained because the methods measure rather the uncertainty from all data and then apply it to single predictions to construct intervals. Therefore, the PIs do not hold a genuine uncertainty of a specific customer. The difference between BS and EN and the rest of the methods is that they are much a lot shorter what causes their low PICP but still, higher CETs will get longer intervals than lower ones. Therefore, when calculating e.g. hul, higher CET values will still have the highest upper limit, regardless of the method and the performance is similar across methods.}

\section{Discussion and conclusion}
Placeholder

\section{Appendix}

\begin{figure}[h]
\centering

\begin{subfigure}{0.5\columnwidth}
\centering
\includegraphics[width=\textwidth]{Plots/90 PIs gift_results.png}
\caption{}
\label{fig:time1}
\end{subfigure}\hfill
\begin{subfigure}{0.5\columnwidth}
\centering
\includegraphics[width=\textwidth]{Plots/90 PIs el_results.png}
\caption{}
\label{fig:time2}
\end{subfigure}

\medskip

\begin{subfigure}{0.5\columnwidth}
\centering
\includegraphics[width=\textwidth]{Plots/90 PIs for multi_results.png}
\caption{}
\label{fig:time3}
\end{subfigure}

\caption{Boxplots}
\label{fig:time}

\end{figure}


%%%%%%%%

\begin{figure}[htp]
\centering

\begin{subfigure}{0.49\columnwidth}
\centering
\includegraphics[width=\textwidth]{Plots/Relative model error for gift_results.png}
\caption{gift data set}
\label{fig:time1}
\end{subfigure}\hfill
\begin{subfigure}{0.49\columnwidth}
\centering
\includegraphics[width=\textwidth]{Plots/Relative model error for el_results.png}
\caption{electronics data set}
\label{fig:time2}
\end{subfigure}

\medskip

\begin{subfigure}{0.49\columnwidth}
\centering
\includegraphics[width=\textwidth]{Plots/Relative model error for multi_results.png}
\caption{multichannel data set}
\label{fig:time3}
\end{subfigure}\hfill
\begin{subfigure}{0.49\columnwidth}
\centering
\includegraphics[width=\textwidth]{Plots/Relative model error for apparel_results.png}
\caption{apparel data set}
\label{fig:time4}
\end{subfigure}

\caption{Relative model error}
\label{Relative model error}

\end{figure}







\begin{table}[h!]
    \centering
     % Shift the table 1cm to the left
    \setlength{\tabcolsep}{4pt} % Adjust column separation
    \renewcommand{\arraystretch}{1.2} % Adjust row separation
    \begin{adjustbox}{width=\textwidth}
    \begin{tabular}{|p{1.4cm}|p{1cm}|p{1.1cm}|p{1.4cm}|p{1.4cm}|p{1.8cm}|p{1.2cm}|p{1cm}|p{1.2cm}|p{1.2cm}|p{1.4cm}|}
        \hline
        \makecell[l]{\textbf{Method}} & \makecell[l]{\textbf{PICP}} & \makecell[l]{\textbf{ACE}} & \makecell[l]{\textbf{PICPW}} & \makecell[l]{\textbf{PIARW}} & \makecell[l]{\textbf{PIARWW}} & \makecell[l]{\textbf{MSIS}} & \makecell[l]{\textbf{SWR}} & \makecell[l]{\textbf{Upper}\\ \textbf{cov.}} & \makecell[l]{\textbf{Lower}\\ \textbf{cov.}} & \makecell[l]{\textbf{Time [s]}\\ \textbf{(abs)}}\\ \hline
        BS & 0.7186 & 0.0100 & 0.5670 & 0.3429 & 0.4499 & 0.7399 & 1.0065 & 0.1195 & 0.3521 & 0.2563  \\ \hline
        EN & 1.1016 & 0.0119 & 0.8428 & 0.6485 & 0.6157 & 0.7453 & 1.3391 & 0.1478 & 0.5582 & 0.9112  \\ \hline
        BA & 0.0484 & -0.8889 & 0.2551 & 0.4371 & 0.3869 & 1.1337 & 0.4004 & 0.0486 & 0.0022 & 1.7258  \\ \hline
        QR & 0.0341 & -0.7262 & 0.1232 & 0.7282 & 0.6986 & 0.7915 & 0.8413 & 0.0336 & 0.0005 & 0.3898  \\ \hline
        CP & 0.0359 & -1.1896 & 0.2931 & 0.6108 & 0.5624 & 0.8914 & 0.7134 & 0.0282 & 0.0074 & 1.7377  \\ \hline
        CR & 0.0432 & -2.5499 & 0.3292 & 0.5823 & 0.5379 & 0.9003 & 0.7050 & 0.0388 & 0.0073 & 1.6915  \\ \hline
        \end{tabular}
    \end{adjustbox}{}    
    \caption{Variation table}
    \label{Variation table}
\end{table}




\pagebreak
\nocite{*}
\bibliographystyle{abbrv}
\bibliography{sample}
\bibliographystyle{plainurl}

\end{document}